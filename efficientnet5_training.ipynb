{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_google_landmark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFFOFGDELtTiRCu1P+/6dS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josemoti1999/google_landmark_retreival_kaggle/blob/master/efficientnet5_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jng9Ppl2rNh"
      },
      "source": [
        "# MCBRIDE SCRIPT Experiment 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMn7gTh4gWgs",
        "outputId": "1db60c1b-e439-4fe9-d3e3-f64d373de7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOp0Qhzjhwr"
      },
      "source": [
        "colab = 1\n",
        "FOLDER = 'Experiment_18'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s89_elIbgW63"
      },
      "source": [
        "!pip install -q efficientnet\n",
        "!pip install -q gcsfs\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import efficientnet.tfkeras as efn\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.notebook import tqdm as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEZqHjd7gf2x"
      },
      "source": [
        "!pip install -q tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
        "import requests\n",
        "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
        "if resp.status_code != 200:\n",
        "  print(\"Failed to switch the TPU to TF {}\".format(version))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVVCTLwajxph",
        "outputId": "80d21b3e-aa37-41e4-998f-62d60064534e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "PATH = '/content/drive/My Drive/Kaggle/landmark_recognition'\n",
        "PATH = os.path.join(PATH, FOLDER)\n",
        "os.makedirs(PATH, exist_ok=True)\n",
        "PATH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Kaggle/landmark_recognition/Experiment_18'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVWI91XSgi--",
        "outputId": "cc9d5509-bc75-40d3-b746-bff9abe5e0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.83.147.250:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.83.147.250:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.83.147.250:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz50NKpagyJG"
      },
      "source": [
        "GCS_PATH = 'gs://kds-4c1ac9004efd00b8a8bcfa9b3dcc800dbdf8978d612363a73d982469'\n",
        "GCS_PATH_2 = 'gs://kds-7850820fffb4d34978de60497a6b764639bd2c8ff0405571fcdb6582'\n",
        "DICT_PATH = 'gs://kds-a322da0692cb29e85f28040e32919375696ed752764db14e715ded3f/train_encoded.csv'\n",
        "\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [384, 384]\n",
        "SEED = 100\n",
        "LR = 3e-5\n",
        "NUMBER_OF_CLASSES = 81313"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woTwitTng09F",
        "outputId": "cf788315-ba51-46c3-9d60-d58d5e5610da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH_2 + '/train*.tfrec')\n",
        "df = pd.read_csv(DICT_PATH)\n",
        "TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(FILENAMES, test_size = 0.02, random_state = SEED)\n",
        "training_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in TRAINING_FILENAMES]\n",
        "validation_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in VALIDATION_FILENAMES]\n",
        "n_trn_classes = df[df['group'].isin(training_groups)]['landmark_id_encode'].nunique()\n",
        "n_val_classes = df[df['group'].isin(validation_groups)]['landmark_id_encode'].nunique()\n",
        "print('Validation tfrecord-',validation_groups)\n",
        "print(f'The number of unique training classes is {n_trn_classes} of {NUMBER_OF_CLASSES} total classes')\n",
        "print(f'The number of unique validation classes is {n_val_classes} of {NUMBER_OF_CLASSES} total classes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation tfrecord- [14]\n",
            "The number of unique training classes is 81313 of 81313 total classes\n",
            "The number of unique validation classes is 24574 of 81313 total classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVhgTrVMg62c"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfSRp6l7hwoD",
        "outputId": "e25882db-9853-4758-8df1-02ac4d203ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    #temp = tf.unstack(image, axis=-1)\n",
        "    #image = tf.stack([temp[2], temp[1], temp[0]], axis=-1)\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    target = tf.cast(example['target'], tf.int32)\n",
        "    return image, target\n",
        "\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "def arcface_format(image, target):\n",
        "    return {'inp1': image, 'inp2': target}, target\n",
        "\n",
        "def get_training_dataset(filenames, ordered = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.repeat() \n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(filenames, ordered = True, prediction = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    if prediction:\n",
        "        dataset = dataset.batch(BATCH_SIZE * 4)\n",
        "    else:\n",
        "        dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    records = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    df = pd.read_csv(DICT_PATH)\n",
        "    n = df[df['group'].isin(records)].shape[0]\n",
        "    return n\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "NUM_VALIDATION_IMAGES  = count_data_items(VALIDATION_FILENAMES)\n",
        "print(f'Training with {NUM_TRAINING_IMAGES} images')\n",
        "print(f'Validating with {NUM_VALIDATION_IMAGES} images')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with 1548860 images\n",
            "Validating with 31610 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx0s3rDUhwq-"
      },
      "source": [
        "def get_lr_callback():\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.0000005 * BATCH_SIZE\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.8\n",
        "     \n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
        "    return lr_callback\n",
        "\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAyG3DlBhwla"
      },
      "source": [
        "def get_model():\n",
        "    with strategy.scope():\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = NUMBER_OF_CLASSES, \n",
        "            s = 64, \n",
        "            m = 0.1, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        x0 = efn.EfficientNetB5(weights = 'noisy-student', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x0)\n",
        "        x = margin([x, label])\n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "        #opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate = 1e-3, momentum=0.99)\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Y0gShjXrQb",
        "outputId": "849f78e5-8857-43f3-d810-8a12a2ad5a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df=df.sort_values(by='landmark_id_encode', ascending=True)\n",
        "y = df['landmark_id_encode'].values\n",
        "weight_map = 1/np.log(np.bincount(y))\n",
        "weight_map = dict(enumerate(weight_map))\n",
        "print(weight_map[55807],weight_map[50843],weight_map[44791],weight_map[6966]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.11436608996297955 0.12969823432482586 1.4426950408889634 1.4426950408889634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSNupGPA9Z14"
      },
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# df=df.sort_values(by='landmark_id_encode', ascending=True)\n",
        "# y = df['landmark_id_encode'].values\n",
        "# classes = df['landmark_id_encode'].unique()\n",
        "# weight_map = compute_class_weight('balanced', classes, y)\n",
        "# weight_map = dict(enumerate(weight_map))\n",
        "# print(weight_map[55807],weight_map[50843],weight_map[44791],weight_map[6966])   \n",
        "# #0.003,0.008,9.71,9.71"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC-1zMJr35D9"
      },
      "source": [
        "#change 3 explicitly\n",
        "EPOCHS = 20\n",
        "SCALE = 64\n",
        "MARGIN = 0.1\n",
        "EFNET = 5\n",
        "RETRAIN = True\n",
        "REDUCE_LR = True\n",
        "MODEL_PATH = 'fold-0_epoch-5_valloss-1.3085_loss-0.0085_margin-0.10_scale-64_logweight.h5'\n",
        "INITIAL_EPOCH = 0\n",
        "WEIGHT_MAP = weight_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH12PcQCisVT",
        "outputId": "36a94903-8c27-42b3-f451-8de9ffb43a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
        "val_dataset = get_validation_dataset(VALIDATION_FILENAMES, ordered = True, prediction = False)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "model = get_model()\n",
        "if RETRAIN:\n",
        "    print('Loading model...')\n",
        "    model.load_weights(os.path.join(PATH, MODEL_PATH))\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(PATH, 'fold-0_epoch-{epoch}_valloss-{val_loss:.4f}_loss-{loss:.4f}_margin-0.10_scale-64_logweight.h5'), \n",
        "                                                 monitor = 'val_loss', \n",
        "                                                 save_best_only = False, \n",
        "                                                 save_weights_only = False)\n",
        "if REDUCE_LR:\n",
        "    cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', \n",
        "                                                        mode = 'min', \n",
        "                                                        factor = 0.8, \n",
        "                                                        patience = 2, \n",
        "                                                        verbose = 0, \n",
        "                                                        min_delta = 0.0001)\n",
        "else:\n",
        "    cb_lr_schedule = get_lr_callback()\n",
        "history = model.fit(train_dataset,  \n",
        "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                    epochs = EPOCHS,\n",
        "                    initial_epoch=INITIAL_EPOCH,\n",
        "                    callbacks = [cb_lr_schedule, checkpoint],\n",
        "                    validation_data = val_dataset,\n",
        "                    class_weight = WEIGHT_MAP,\n",
        "                    verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_noisy-student_notop.h5\n",
            "115261440/115255328 [==============================] - 1s 0us/step\n",
            "Loading model...\n",
            "Epoch 1/20\n",
            "12100/12100 [==============================] - 6426s 531ms/step - loss: 0.0083 - sparse_categorical_accuracy: 0.9964 - val_loss: 1.3121 - val_sparse_categorical_accuracy: 0.8065 - lr: 0.0010\n",
            "Epoch 2/20\n",
            " 7324/12100 [=================>............] - ETA: 41:20 - loss: 0.0084 - sparse_categorical_accuracy: 0.9964"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMQntnA9Yn2X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}